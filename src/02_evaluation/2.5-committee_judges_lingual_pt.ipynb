{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7ff4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba47c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_FILES = {\n",
    "    'claude': '../../data/judged/sample_pt_claude_judge_lingual.csv',\n",
    "    'prometheus': '../../data/judged/sample_pt_prometheus_judge_lingual.csv',\n",
    "    'mistral': '../../data/judged/sample_pt_mistral_judge_lingual.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ba953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando arquivo base: claude\n",
      "✓ Carregado: 500 registros\n"
     ]
    }
   ],
   "source": [
    "primeiro_judge = list(JUDGE_FILES.keys())[0]\n",
    "primeiro_arquivo = JUDGE_FILES[primeiro_judge]\n",
    "\n",
    "print(f\"\\nCarregando arquivo base: {primeiro_judge}\")\n",
    "df_analysis = pd.read_csv(primeiro_arquivo)\n",
    "print(f\"✓ Carregado: {len(df_analysis)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850318a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>response_A</th>\n",
       "      <th>response_B</th>\n",
       "      <th>evaluation_claude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>{\\n    \"winner\": \"A\",\\n    \"general_justificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>**Notação Big O: uma medida de complexidade**\\...</td>\n",
       "      <td>A notação Big O é uma medida de complexidade d...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG004_gemini-1.5-pro-latest_1_General Knowledg...</td>\n",
       "      <td>A Proclamação da República no Brasil em 15 de ...</td>\n",
       "      <td>A Proclamação da República no Brasil, em 15 de...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CR019_gpt-4o_3_Creative_detailed_en_vs_pt</td>\n",
       "      <td>Título: \"A Bússola do Coração\"\\n\\nConceito:\\n\\...</td>\n",
       "      <td>**Título: Bússola do Coração**\\n\\n**Conceito:*...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TC007_llama-3.3-70b-versatile_4_Technical_stru...</td>\n",
       "      <td>A compreensão de lista em Python é uma maneira...</td>\n",
       "      <td>Resposta: \"Uma 'list comprehension' em Python ...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...   \n",
       "2  CG004_gemini-1.5-pro-latest_1_General Knowledg...   \n",
       "3          CR019_gpt-4o_3_Creative_detailed_en_vs_pt   \n",
       "4  TC007_llama-3.3-70b-versatile_4_Technical_stru...   \n",
       "\n",
       "                                          response_A  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  **Notação Big O: uma medida de complexidade**\\...   \n",
       "2  A Proclamação da República no Brasil em 15 de ...   \n",
       "3  Título: \"A Bússola do Coração\"\\n\\nConceito:\\n\\...   \n",
       "4  A compreensão de lista em Python é uma maneira...   \n",
       "\n",
       "                                          response_B  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  A notação Big O é uma medida de complexidade d...   \n",
       "2  A Proclamação da República no Brasil, em 15 de...   \n",
       "3  **Título: Bússola do Coração**\\n\\n**Conceito:*...   \n",
       "4  Resposta: \"Uma 'list comprehension' em Python ...   \n",
       "\n",
       "                                   evaluation_claude  \n",
       "0  {\\n    \"winner\": \"A\",\\n    \"general_justificat...  \n",
       "1  {\\n    \"winner\": \"B\",\\n    \"general_justificat...  \n",
       "2  {\\n    \"winner\": \"B\",\\n    \"general_justificat...  \n",
       "3  {\\n    \"winner\": \"B\",\\n    \"general_justificat...  \n",
       "4  {\\n    \"winner\": \"B\",\\n    \"general_justificat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff77b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from judge 'prometheus' successfully merged.\n",
      "Data from judge 'mistral' successfully merged.\n",
      "\n",
      "Shape of the DataFrame after merging: (500, 6)\n",
      "Sample of raw data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>response_A</th>\n",
       "      <th>response_B</th>\n",
       "      <th>evaluation_claude</th>\n",
       "      <th>evaluation_prometheus</th>\n",
       "      <th>evaluation_mistral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>{\\n    \"winner\": \"A\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"T\",\\n\"general_justification\": \"T...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>**Notação Big O: uma medida de complexidade**\\...</td>\n",
       "      <td>A notação Big O é uma medida de complexidade d...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"Tie\",\\n\"general_justification\": ...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG004_gemini-1.5-pro-latest_1_General Knowledg...</td>\n",
       "      <td>A Proclamação da República no Brasil em 15 de ...</td>\n",
       "      <td>A Proclamação da República no Brasil, em 15 de...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"T\",\\n\"general_justification\": \"B...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...   \n",
       "2  CG004_gemini-1.5-pro-latest_1_General Knowledg...   \n",
       "\n",
       "                                          response_A  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  **Notação Big O: uma medida de complexidade**\\...   \n",
       "2  A Proclamação da República no Brasil em 15 de ...   \n",
       "\n",
       "                                          response_B  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  A notação Big O é uma medida de complexidade d...   \n",
       "2  A Proclamação da República no Brasil, em 15 de...   \n",
       "\n",
       "                                   evaluation_claude  \\\n",
       "0  {\\n    \"winner\": \"A\",\\n    \"general_justificat...   \n",
       "1  {\\n    \"winner\": \"B\",\\n    \"general_justificat...   \n",
       "2  {\\n    \"winner\": \"B\",\\n    \"general_justificat...   \n",
       "\n",
       "                               evaluation_prometheus  \\\n",
       "0  {\\n\"winner\": \"T\",\\n\"general_justification\": \"T...   \n",
       "1  {\\n\"winner\": \"Tie\",\\n\"general_justification\": ...   \n",
       "2  {\\n\"winner\": \"T\",\\n\"general_justification\": \"B...   \n",
       "\n",
       "                                  evaluation_mistral  \n",
       "0   {\\n        \"winner\": \"A\",\\n        \"general_j...  \n",
       "1   {\\n        \"winner\": \"A\",\\n        \"general_j...  \n",
       "2   {\\n        \"winner\": \"A\",\\n        \"general_j...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for judge_name, filename in list(JUDGE_FILES.items())[1:]:\n",
    "    try:\n",
    "        df_judge = pd.read_csv(filename, usecols=['evaluation_id', f'evaluation_{judge_name}'])\n",
    "        df_analysis = pd.merge(df_analysis, df_judge, on='evaluation_id', how='left')\n",
    "        print(f\"Data from judge '{judge_name}' successfully merged.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing the file for '{judge_name}': {e}\")\n",
    "\n",
    "print(\"\\nShape of the DataFrame after merging:\", df_analysis.shape)\n",
    "print(\"Sample of raw data:\")\n",
    "df_analysis.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136664c",
   "metadata": {},
   "source": [
    "### PROCESS CLAUDE EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5387f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['claude_winner'] = None\n",
    "df_analysis['claude_total_score'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870c84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_markdown(text):\n",
    "    \"\"\"Extrai JSON de blocos markdown ```json...```\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "\n",
    "    match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', str(text), re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf98dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude processing completed.\n",
      "Sample results for Claude:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>claude_winner</th>\n",
       "      <th>claude_total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>A</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>B</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG004_gemini-1.5-pro-latest_1_General Knowledg...</td>\n",
       "      <td>B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CR019_gpt-4o_3_Creative_detailed_en_vs_pt</td>\n",
       "      <td>B</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TC007_llama-3.3-70b-versatile_4_Technical_stru...</td>\n",
       "      <td>B</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id claude_winner  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...             A   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...             B   \n",
       "2  CG004_gemini-1.5-pro-latest_1_General Knowledg...             B   \n",
       "3          CR019_gpt-4o_3_Creative_detailed_en_vs_pt             B   \n",
       "4  TC007_llama-3.3-70b-versatile_4_Technical_stru...             B   \n",
       "\n",
       "   claude_total_score  \n",
       "0                4.50  \n",
       "1                4.75  \n",
       "2                5.00  \n",
       "3                4.50  \n",
       "4                3.75  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_analysis.iterrows():\n",
    "    try:\n",
    "        # Extract JSON from markdown first\n",
    "        json_text = extract_json_from_markdown(row['evaluation_claude'])\n",
    "        \n",
    "        if json_text is None:\n",
    "            raise ValueError(\"Empty evaluation\")\n",
    "        \n",
    "        data = json.loads(json_text)\n",
    "        winner = data.get('winner')\n",
    "        criteria = data.get('criteria', {})\n",
    "        df_analysis.loc[index, 'claude_winner'] = winner\n",
    "\n",
    "        scores = []\n",
    "        if criteria:\n",
    "            for crit_details in criteria.values():\n",
    "                if 'score' in crit_details:\n",
    "                    scores.append(crit_details['score'])\n",
    "                elif 'score_a' in crit_details:\n",
    "                    scores.append(crit_details['score_a'])\n",
    "                elif 'score_b' in crit_details:\n",
    "                    scores.append(crit_details['score_b'])\n",
    "                elif 'score_A' in crit_details:\n",
    "                    scores.append(crit_details['score_A'])\n",
    "                elif 'score_B' in crit_details:\n",
    "                    scores.append(crit_details['score_B'])\n",
    "\n",
    "        if scores:\n",
    "            df_analysis.loc[index, 'claude_total_score'] = np.mean(scores)\n",
    "        else:\n",
    "            df_analysis.loc[index, 'claude_total_score'] = 0\n",
    "    except (TypeError, json.JSONDecodeError, ValueError) as e:\n",
    "        df_analysis.loc[index, 'claude_winner'] = 'Parsing Error'\n",
    "        df_analysis.loc[index, 'claude_total_score'] = 0\n",
    "\n",
    "print(\"Claude processing completed.\")\n",
    "print(\"Sample results for Claude:\")\n",
    "df_analysis[['evaluation_id', 'claude_winner', 'claude_total_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf416e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where 'claude_total_score' == 0: 0\n",
      "Count where 'claude_winner' is NaN: 0\n",
      "Count where 'claude_winner' == 'Parsing Error': 0\n"
     ]
    }
   ],
   "source": [
    "score_zero_count = (df_analysis['claude_total_score'] == 0).sum()\n",
    "print(f\"Count where 'claude_total_score' == 0: {score_zero_count}\")\n",
    "\n",
    "winner_nan_count = df_analysis['claude_winner'].isna().sum()\n",
    "print(f\"Count where 'claude_winner' is NaN: {winner_nan_count}\")\n",
    "\n",
    "parsing_error_count = (df_analysis['claude_winner'] == 'Parsing Error').sum()\n",
    "print(f\"Count where 'claude_winner' == 'Parsing Error': {parsing_error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc93b68d",
   "metadata": {},
   "source": [
    "### PROCESS MISTRAL EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a25a6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['mistral_winner'] = None\n",
    "df_analysis['mistral_total_score'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db9aa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral processing completed.\n",
      "\n",
      "Sample results for Mistral:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>mistral_winner</th>\n",
       "      <th>mistral_total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>A</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>A</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG004_gemini-1.5-pro-latest_1_General Knowledg...</td>\n",
       "      <td>A</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CR019_gpt-4o_3_Creative_detailed_en_vs_pt</td>\n",
       "      <td>A</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TC007_llama-3.3-70b-versatile_4_Technical_stru...</td>\n",
       "      <td>A</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id mistral_winner  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...              A   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...              A   \n",
       "2  CG004_gemini-1.5-pro-latest_1_General Knowledg...              A   \n",
       "3          CR019_gpt-4o_3_Creative_detailed_en_vs_pt              A   \n",
       "4  TC007_llama-3.3-70b-versatile_4_Technical_stru...              A   \n",
       "\n",
       "   mistral_total_score  \n",
       "0                 4.75  \n",
       "1                 5.00  \n",
       "2                 4.75  \n",
       "3                 4.25  \n",
       "4                 5.00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df_analysis.iterrows():\n",
    "    eval_text = str(row['evaluation_mistral'])\n",
    "    \n",
    "    json_list = []\n",
    "    \n",
    "    try:\n",
    "        # ATTEMPT 1: Try to read as a single valid JSON\n",
    "        data = json.loads(eval_text)\n",
    "        json_list.append(data)\n",
    "\n",
    "    except (TypeError, json.JSONDecodeError):\n",
    "        try:\n",
    "            # ATTEMPT 2: If the first fails, fix text using a SPACE separator\n",
    "            fixed_text = '[' + re.sub(r'\\}\\s+\\{', '}, {', eval_text) + ']'\n",
    "            json_list = json.loads(fixed_text)\n",
    "        except (TypeError, json.JSONDecodeError):\n",
    "            try:\n",
    "                # ATTEMPT 3: Split JSONs using a more specific pattern\n",
    "                # Look for patterns like \"} [RESPONSE B] {\" or \"}\\n\\n{\"\n",
    "                fixed_text = re.sub(r'\\}\\s*\\[RESPONSE\\s+[A-Z]\\]\\s*\\{', '}, {', eval_text, flags=re.IGNORECASE)\n",
    "                fixed_text = '[' + re.sub(r'\\}\\s*\\n\\s*\\{', '}, {', fixed_text) + ']'\n",
    "                json_list = json.loads(fixed_text)\n",
    "            except (TypeError, json.JSONDecodeError):\n",
    "                try:\n",
    "                    # ATTEMPT 4: Use a more generic JSON separator\n",
    "                    fixed_text = '[' + re.sub(r'\\}\\s*\\{', '}, {', eval_text) + ']'\n",
    "                    json_list = json.loads(fixed_text)\n",
    "                except (TypeError, json.JSONDecodeError):\n",
    "                    try:\n",
    "                        # ATTEMPT 5: Extract valid JSONs using brace counting\n",
    "                        json_objects = []\n",
    "                        i = 0\n",
    "                        \n",
    "                        while i < len(eval_text):\n",
    "                            if eval_text[i] == '{':\n",
    "                                brace_count = 0\n",
    "                                start = i\n",
    "                                in_string = False\n",
    "                                escape = False\n",
    "                                \n",
    "                                while i < len(eval_text):\n",
    "                                    char = eval_text[i]\n",
    "                                    \n",
    "                                    if char == '\"' and not escape:\n",
    "                                        in_string = not in_string\n",
    "                                    elif char == '\\\\' and not escape:\n",
    "                                        escape = True\n",
    "                                        i += 1\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if not in_string:\n",
    "                                        if char == '{':\n",
    "                                            brace_count += 1\n",
    "                                        elif char == '}':\n",
    "                                            brace_count -= 1\n",
    "                                            if brace_count == 0:\n",
    "                                                json_str = eval_text[start:i+1]\n",
    "                                                try:\n",
    "                                                    obj = json.loads(json_str)\n",
    "                                                    if 'winner' in obj and 'criteria' in obj:\n",
    "                                                        json_objects.append(obj)\n",
    "                                                except json.JSONDecodeError:\n",
    "                                                    pass\n",
    "                                                break\n",
    "                                    \n",
    "                                    escape = False\n",
    "                                    i += 1\n",
    "                            i += 1\n",
    "                        \n",
    "                        if json_objects:\n",
    "                            json_list = json_objects\n",
    "                        else:\n",
    "                            raise json.JSONDecodeError(\"No valid JSON found\", eval_text, 0)\n",
    "                            \n",
    "                    except (TypeError, json.JSONDecodeError):\n",
    "                        try:\n",
    "                            # ATTEMPT 6: Handle truncated JSON by finding last valid one\n",
    "                            json_objects = []\n",
    "                            \n",
    "                            for end_pos in range(len(eval_text) - 1, -1, -1):\n",
    "                                if eval_text[end_pos] == '}':\n",
    "                                    test_text = eval_text[:end_pos + 1]\n",
    "                                    \n",
    "                                    brace_count = 0\n",
    "                                    for start_pos in range(end_pos, -1, -1):\n",
    "                                        if test_text[start_pos] == '}':\n",
    "                                            brace_count += 1\n",
    "                                        elif test_text[start_pos] == '{':\n",
    "                                            brace_count -= 1\n",
    "                                            \n",
    "                                            if brace_count == 0:\n",
    "                                                json_str = test_text[start_pos:end_pos + 1]\n",
    "                                                try:\n",
    "                                                    obj = json.loads(json_str)\n",
    "                                                    if isinstance(obj, dict) and 'winner' in obj and 'criteria' in obj:\n",
    "                                                        json_objects.append(obj)\n",
    "                                                        break\n",
    "                                                except json.JSONDecodeError:\n",
    "                                                    continue\n",
    "                            \n",
    "                            if json_objects:\n",
    "                                json_list = json_objects\n",
    "                            else:\n",
    "                                raise json.JSONDecodeError(\"No valid JSON found\", eval_text, 0)\n",
    "                                \n",
    "                        except (TypeError, json.JSONDecodeError):\n",
    "                            try:\n",
    "                                # ATTEMPT 7: Fix unbalanced braces in truncated JSON\n",
    "                                open_braces = eval_text.count('{')\n",
    "                                close_braces = eval_text.count('}')\n",
    "                                if open_braces > close_braces:\n",
    "                                    fixed_text = eval_text + ('}' * (open_braces - close_braces))\n",
    "                                elif close_braces > open_braces:\n",
    "                                    fixed_text = eval_text[:-(close_braces - open_braces)]\n",
    "                                else:\n",
    "                                    fixed_text = eval_text\n",
    "\n",
    "                                data = json.loads(fixed_text)\n",
    "                                if isinstance(data, dict) and 'winner' in data and 'criteria' in data:\n",
    "                                    json_list.append(data)\n",
    "                                elif isinstance(data, list):\n",
    "                                    json_list.extend(data)\n",
    "\n",
    "                            except (TypeError, json.JSONDecodeError):\n",
    "                                try:\n",
    "                                    # ATTEMPT 8: Final fallback — fix broken quotes inside JSON strings\n",
    "                                    fixed_text = eval_text\n",
    "\n",
    "                                    # Escape internal double quotes\n",
    "                                    fixed_text = re.sub(r'(?<!\\\\)\"(?![:,}\\]\\s])', r'\\\\\"', fixed_text)\n",
    "                                    # Fix duplicated quotes\n",
    "                                    fixed_text = re.sub(r'\\\\\"{2,}', r'\\\\\"', fixed_text)\n",
    "                                    fixed_text = re.sub(r'\"\"', '\"', fixed_text)\n",
    "                                    # Remove invalid commas between quotes\n",
    "                                    fixed_text = re.sub(r'\"\\s*,\\s*\"', ' ', fixed_text)\n",
    "                                    # Remove unnecessary line breaks\n",
    "                                    fixed_text = fixed_text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                                    # Balance braces\n",
    "                                    open_braces = fixed_text.count('{')\n",
    "                                    close_braces = fixed_text.count('}')\n",
    "                                    if open_braces > close_braces:\n",
    "                                        fixed_text += '}' * (open_braces - close_braces)\n",
    "                                    # Balance quotes\n",
    "                                    double_quotes = fixed_text.count('\"')\n",
    "                                    if double_quotes % 2 != 0:\n",
    "                                        fixed_text += '\"'\n",
    "\n",
    "                                    # Final parse attempt\n",
    "                                    data = json.loads(fixed_text)\n",
    "\n",
    "                                    if isinstance(data, dict) and 'winner' in data and 'criteria' in data:\n",
    "                                        json_list.append(data)\n",
    "                                    elif isinstance(data, list):\n",
    "                                        json_list.extend(data)\n",
    "\n",
    "                                except (TypeError, json.JSONDecodeError) as e:\n",
    "                                    print(f\"Final failure at index {index}: {e}\")\n",
    "                                    df_analysis.loc[index, 'mistral_winner'] = 'Parsing Error'\n",
    "                                    df_analysis.loc[index, 'mistral_total_score'] = 0\n",
    "                                    continue\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for data in json_list:\n",
    "        winner = data.get('winner')\n",
    "        criteria = data.get('criteria', {})\n",
    "        \n",
    "        scores = []\n",
    "        if criteria:\n",
    "            for crit_details in criteria.values():\n",
    "                score = (\n",
    "                    crit_details.get('score') or \n",
    "                    crit_details.get('score_A') or \n",
    "                    crit_details.get('score_a') or \n",
    "                    crit_details.get('score_B') or \n",
    "                    crit_details.get('score_b')\n",
    "                )\n",
    "                if score is not None:\n",
    "                    try:\n",
    "                        scores.append(float(score))\n",
    "                    except (ValueError, TypeError):\n",
    "                        continue\n",
    "        \n",
    "        avg_score = np.mean(scores) if scores else 0\n",
    "        \n",
    "        if winner and avg_score > 0:\n",
    "            results.append((winner, avg_score))\n",
    "\n",
    "    # Select the result with the highest score\n",
    "    if results:\n",
    "        best_winner, best_score = max(results, key=lambda item: item[1])\n",
    "        df_analysis.loc[index, 'mistral_winner'] = best_winner\n",
    "        df_analysis.loc[index, 'mistral_total_score'] = best_score\n",
    "    else:\n",
    "        df_analysis.loc[index, 'mistral_winner'] = 'Parsing Error (No valid JSON found)'\n",
    "        df_analysis.loc[index, 'mistral_total_score'] = 0\n",
    "\n",
    "\n",
    "print(\"Mistral processing completed.\")\n",
    "print(\"\\nSample results for Mistral:\")\n",
    "df_analysis[['evaluation_id', 'mistral_winner', 'mistral_total_score']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b9aad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where 'mistral_total_score' == 0: 0\n",
      "Count where 'mistral_winner' is NaN: 0\n",
      "Count where 'mistral_winner' == 'Parsing Error': 0\n"
     ]
    }
   ],
   "source": [
    "score_zero_count = (df_analysis['mistral_total_score'] == 0).sum()\n",
    "print(f\"Count where 'mistral_total_score' == 0: {score_zero_count}\")\n",
    "\n",
    "winner_nan_count = df_analysis['mistral_winner'].isna().sum()\n",
    "print(f\"Count where 'mistral_winner' is NaN: {winner_nan_count}\")\n",
    "\n",
    "parsing_error_count = (df_analysis['mistral_winner'] == 'Parsing Error').sum()\n",
    "print(f\"Count where 'mistral_winner' == 'Parsing Error': {parsing_error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f868317",
   "metadata": {},
   "source": [
    "### PROCESS PROMETHEUS EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d0245ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['prometheus_winner'] = None\n",
    "df_analysis['prometheus_total_score'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb375c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prometheus processing completed.\n",
      "Sample results for Prometheus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>prometheus_winner</th>\n",
       "      <th>prometheus_total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>T</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>Tie</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG004_gemini-1.5-pro-latest_1_General Knowledg...</td>\n",
       "      <td>T</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CR019_gpt-4o_3_Creative_detailed_en_vs_pt</td>\n",
       "      <td>B</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TC007_llama-3.3-70b-versatile_4_Technical_stru...</td>\n",
       "      <td>A</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id prometheus_winner  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...                 T   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...               Tie   \n",
       "2  CG004_gemini-1.5-pro-latest_1_General Knowledg...                 T   \n",
       "3          CR019_gpt-4o_3_Creative_detailed_en_vs_pt                 B   \n",
       "4  TC007_llama-3.3-70b-versatile_4_Technical_stru...                 A   \n",
       "\n",
       "   prometheus_total_score  \n",
       "0                    5.00  \n",
       "1                    4.75  \n",
       "2                    4.75  \n",
       "3                    5.00  \n",
       "4                    4.75  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "score_patterns = [\n",
    "    r'\"score\"\\s*:\\s*(\\d+)',\n",
    "    r'score:\\s*(\\d+)',\n",
    "    r'\"score\"\\s*(\\d+)',\n",
    "    r'\"logical_coherence\"\\s*:\\s*(\\d+)',\n",
    "    r'\"relevance_and_focus\"\\s*:\\s*(\\d+)',\n",
    "    r'\"accuracy_and_truthfulness\"\\s*:\\s*(\\d+)',\n",
    "    r'\"conciseness_and_clarity\"\\s*:\\s*(\\d+)',\n",
    "    r'Logical Coherence:\\s*(\\d+)',\n",
    "    r'Relevance and Focus:\\s*(\\d+)',\n",
    "    r'Accuracy and Truthfulness:\\s*(\\d+)',\n",
    "    r'Conciseness and Clarity:\\s*(\\d+)',\n",
    "    r'\\*\\s*Logical Coherence:\\s*(\\d+)',\n",
    "    r'\\*\\s*Relevance and Focus:\\s*(\\d+)',\n",
    "    r'\\*\\s*Accuracy and Truthfulness:\\s*(\\d+)',\n",
    "    r'\\*\\s*Conciseness and Clarity:\\s*(\\d+)',\n",
    "    r'logical_coherence[\"\\']?\\s*:\\s*\\{\\s*[\"\\']?score[\"\\']?\\s*:\\s*(\\d+)',\n",
    "    r'relevance_and_focus[\"\\']?\\s*:\\s*\\{\\s*[\"\\']?score[\"\\']?\\s*:\\s*(\\d+)',\n",
    "    r'accuracy_and_truthfulness[\"\\']?\\s*:\\s*\\{\\s*[\"\\']?score[\"\\']?\\s*:\\s*(\\d+)',\n",
    "    r'conciseness_and_clarity[\"\\']?\\s*:\\s*\\{\\s*[\"\\']?score[\"\\']?\\s*:\\s*(\\d+)',\n",
    "    r'scores?\\s+(\\d+)',\n",
    "    r'\\(0-5\\):\\s*Response [AB] scores?\\s+(\\d+)',\n",
    "]\n",
    "\n",
    "for index, row in df_analysis.iterrows():\n",
    "    eval_text = str(row['evaluation_prometheus'])\n",
    "    \n",
    "    # --- FIRST ATTEMPT: COMPLETE JSON ---\n",
    "    json_parsed_successfully = False\n",
    "    try:\n",
    "        data = None\n",
    "        # Try loading the entire string first. This is the cleanest case.\n",
    "        try:\n",
    "            data = json.loads(eval_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # If it fails, search for the JSON substring more intelligently\n",
    "            start = eval_text.find('{')\n",
    "            end = eval_text.rfind('}') + 1\n",
    "            if start != -1 and end != 0:\n",
    "                json_str = eval_text[start:end]\n",
    "                data = json.loads(json_str)\n",
    "\n",
    "        # If 'data' was successfully loaded either way\n",
    "        if data:\n",
    "            winner_val = data.get('winner', 'Parsing Error')\n",
    "\n",
    "            if winner_val == 'Parsing Error':\n",
    "                match = re.search(r'\"winner\"\\s*:\\s*\"(Tie|A|B)\"', eval_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    winner_val = match.group(1)\n",
    "\n",
    "            if isinstance(winner_val, str):\n",
    "                winner_val = winner_val.capitalize() if winner_val.lower() == 'tie' else winner_val.upper()\n",
    "            \n",
    "            criteria = data.get('criteria', {})\n",
    "            scores = [d.get('score') for d in criteria.values() if isinstance(d, dict) and d.get('score') is not None]\n",
    "            valid_scores = [int(s) for s in scores if isinstance(s, (int, float)) and 0 <= int(s) <= 5]\n",
    "            \n",
    "            df_analysis.loc[index, 'prometheus_winner'] = winner_val\n",
    "            df_analysis.loc[index, 'prometheus_total_score'] = np.mean(valid_scores) if valid_scores else 0\n",
    "            \n",
    "            if winner_val != 'Parsing Error':\n",
    "                continue\n",
    "\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # If it's not valid JSON or there's an error, just ignore it and move on to the regex.\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        fixed_text = eval_text.strip()\n",
    "        \n",
    "        # Try to extract the main JSON\n",
    "        json_match = re.search(r'\\{[^{}]*\"winner\"[^{}]*\\}', fixed_text, re.DOTALL)\n",
    "        match = re.search(r'\"winner\"\\s*:\\s*\"(Tie|A|B)\"', fixed_text, re.IGNORECASE)\n",
    "        if json_match:\n",
    "            try:\n",
    "                data = json.loads(json_match.group())\n",
    "                \n",
    "                # Extract winner\n",
    "                winner_val = data.get('winner', 'Parsing Error')\n",
    "\n",
    "                if winner_val == 'Parsing Error':\n",
    "                    match = re.search(r'\"winner\"\\s*:\\s*\"(Tie|A|B)\"', eval_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        winner_val = match.group(1)\n",
    "                        \n",
    "                if isinstance(winner_val, str):\n",
    "                    winner_val = winner_val.capitalize() if winner_val.lower() == 'tie' else winner_val.upper()\n",
    "                df_analysis.loc[index, 'prometheus_winner'] = winner_val\n",
    "                \n",
    "               # Extract scores from criteria\n",
    "                criteria = data.get('criteria', {})\n",
    "                if isinstance(criteria, dict):\n",
    "                    scores = [\n",
    "                        details.get('score') \n",
    "                        for details in criteria.values() \n",
    "                        if isinstance(details, dict) and details.get('score') is not None\n",
    "                    ]\n",
    "                    valid_scores = [int(s) for s in scores if isinstance(s, (int, float)) and 0 <= int(s) <= 5]\n",
    "                    df_analysis.loc[index, 'prometheus_total_score'] = np.mean(valid_scores) if valid_scores else 0\n",
    "                else:\n",
    "                    regex = r'\"score\"\\s*:\\s*(\\d+)'\n",
    "                    scores = re.findall(regex, eval_text)\n",
    "                    scores_numericos = [int(s) for s in scores]\n",
    "\n",
    "                    df_analysis.loc[index, 'prometheus_total_score'] = np.mean(scores_numericos) if scores_numericos else 0\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Attempt to fix and parse malformed JSON\n",
    "        fixed_text = re.sub(r'\"\\s*\"criteria\"', '\", \"criteria\"', fixed_text)\n",
    "        fixed_text = re.sub(r'\\}\\s*\\{', '}, {', fixed_text)\n",
    "        if not fixed_text.strip().startswith('{') and '{' in fixed_text:\n",
    "            fixed_text = '{' + fixed_text.split('{', 1)[-1]\n",
    "        if fixed_text.count('{') > 1 and not fixed_text.strip().startswith('['):\n",
    "            fixed_text = '[' + fixed_text + ']'\n",
    "\n",
    "        data = json.loads(fixed_text)\n",
    "        if isinstance(data, list):\n",
    "            data = data[0] if data else {}\n",
    "\n",
    "        winner_val = data.get('winner', 'Parsing Error')\n",
    "        \n",
    "        if winner_val == 'Parsing Error':\n",
    "                match = re.search(r'\"winner\"\\s*:\\s*\"(Tie|A|B)\"', eval_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    winner_val = match.group(1)\n",
    "\n",
    "        if isinstance(winner_val, str):\n",
    "            winner_val = winner_val.capitalize() if winner_val.lower() == 'tie' else winner_val.upper()\n",
    "        df_analysis.loc[index, 'prometheus_winner'] = winner_val\n",
    "        \n",
    "        criteria = data.get('criteria')\n",
    "        if isinstance(criteria, dict):\n",
    "            scores = [\n",
    "                details.get('score') \n",
    "                for details in criteria.values() \n",
    "                if isinstance(details, dict) and details.get('score') is not None\n",
    "            ]\n",
    "            valid_scores = [int(s) for s in scores if isinstance(s, (int, float)) and 0 <= int(s) <= 5]\n",
    "            df_analysis.loc[index, 'prometheus_total_score'] = np.mean(valid_scores) if valid_scores else 0\n",
    "        else:\n",
    "            regex = r'\"score\"\\s*:\\s*(\\d+)'\n",
    "            scores = re.findall(regex, eval_text)\n",
    "            scores_numericos = [int(s) for s in scores]\n",
    "\n",
    "            df_analysis.loc[index, 'prometheus_total_score'] = np.mean(scores_numericos) if scores_numericos else 0\n",
    "        continue\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # --- SECOND ATTEMPT: Regex \"winner is...\" format ---\n",
    "    winner_match = re.search(r'winner is (?:Response )?[\"\\']?([AB]|Tie)[\"\\']?', eval_text, re.IGNORECASE)\n",
    "    \n",
    "    if winner_match:\n",
    "        winner = winner_match.group(1).capitalize()\n",
    "        df_analysis.loc[index, 'prometheus_winner'] = winner\n",
    "        \n",
    "        # Look for score\n",
    "        score_match = re.search(r'with a score of (\\d+)', eval_text, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            df_analysis.loc[index, 'prometheus_total_score'] = int(score_match.group(1))\n",
    "        else:\n",
    "            # Try \"overall score is X\" or \"score is X\"\n",
    "            overall_match = re.search(r'(?:overall\\s+)?score\\s+is\\s+(\\d+)', eval_text, re.IGNORECASE)\n",
    "            if overall_match:\n",
    "                df_analysis.loc[index, 'prometheus_total_score'] = int(overall_match.group(1))\n",
    "            else:\n",
    "                # Try to extract scores from criteria in the text\n",
    "                scores = []\n",
    "                for pattern in score_patterns:\n",
    "                    matches = re.findall(pattern, eval_text, re.IGNORECASE)\n",
    "                    scores.extend([int(m) for m in matches])\n",
    "                \n",
    "                # Filter valid scores (0-5)\n",
    "                scores = [s for s in scores if 0 <= s <= 5]\n",
    "                \n",
    "                if not scores:\n",
    "                    criteria_mentioned = sum([\n",
    "                        1 for crit in ['logical_coherence', 'relevance_and_focus', \n",
    "                                       'accuracy_and_truthfulness', 'conciseness_and_clarity']\n",
    "                        if re.search(crit.replace('_', r'[_\\s]'), eval_text, re.IGNORECASE)\n",
    "                    ])\n",
    "                    \n",
    "                    if criteria_mentioned > 0:\n",
    "                        positive_words = len(re.findall(r'\\b(comprehensive|detailed|coherent|well-structured|accurate|clear|concise|relevant|focused|logical|excellent|superior|better|good|strong|effective)\\b', eval_text, re.IGNORECASE))\n",
    "                        negative_words = len(re.findall(r'\\b(not|lacking|poor|weak|unclear|inaccurate|irrelevant|inconsistent|insufficient|limited)\\b', eval_text, re.IGNORECASE))\n",
    "                        \n",
    "                        if positive_words > negative_words:\n",
    "                            scores = [5] * criteria_mentioned\n",
    "                        elif positive_words > 0:\n",
    "                            scores = [4] * criteria_mentioned\n",
    "                        else:\n",
    "                            scores = [3] * criteria_mentioned\n",
    "                \n",
    "                if scores:\n",
    "                    df_analysis.loc[index, 'prometheus_total_score'] = np.mean(scores)\n",
    "                else:\n",
    "                    regex = r'\"score\"\\s*:\\s*(\\d+)'\n",
    "                    scores = re.findall(regex, eval_text)\n",
    "                    scores_numericos = [int(s) for s in scores]\n",
    "\n",
    "                    df_analysis.loc[index, 'prometheus_total_score'] = np.mean(scores_numericos) if scores_numericos else 0\n",
    "        \n",
    "        continue\n",
    "\n",
    "    # --- THIRD ATTEMPT: Extract winner from other formats ---\n",
    "    alt_winner = re.search(r'(?:the|overall)\\s+winner\\s+is\\s+[\"\\']?([AB]|Tie)[\"\\']?', eval_text, re.IGNORECASE)\n",
    "    if alt_winner:\n",
    "        df_analysis.loc[index, 'prometheus_winner'] = alt_winner.group(1).upper()\n",
    "        \n",
    "        scores = []\n",
    "        for pattern in score_patterns:\n",
    "            matches = re.findall(pattern, eval_text, re.IGNORECASE)\n",
    "            scores.extend([int(m) for m in matches])\n",
    "        \n",
    "        # Filter valid scores\n",
    "        scores = [s for s in scores if 0 <= s <= 5]\n",
    "        \n",
    "        if not scores:\n",
    "            criteria_mentioned = sum([\n",
    "                1 for crit in ['logical_coherence', 'relevance_and_focus', \n",
    "                               'accuracy_and_truthfulness', 'conciseness_and_clarity']\n",
    "                if re.search(crit.replace('_', r'[_\\s]'), eval_text, re.IGNORECASE)\n",
    "            ])\n",
    "            \n",
    "            if criteria_mentioned > 0:\n",
    "                positive_words = len(re.findall(r'\\b(comprehensive|detailed|coherent|well-structured|accurate|clear|concise|relevant|focused|logical|excellent|superior|better|good|strong|effective)\\b', eval_text, re.IGNORECASE))\n",
    "                negative_words = len(re.findall(r'\\b(not|lacking|poor|weak|unclear|inaccurate|irrelevant|inconsistent|insufficient|limited)\\b', eval_text, re.IGNORECASE))\n",
    "                \n",
    "                if positive_words > negative_words:\n",
    "                    scores = [5] * criteria_mentioned\n",
    "                elif positive_words > 0:\n",
    "                    scores = [4] * criteria_mentioned\n",
    "                else:\n",
    "                    scores = [3] * criteria_mentioned\n",
    "        \n",
    "        df_analysis.loc[index, 'prometheus_total_score'] = np.mean(scores) if scores else 0\n",
    "        continue\n",
    "\n",
    "    # --- FINAL FALLBACK: Extract scores directly from text ---\n",
    "    try:\n",
    "        winner_fallback = re.search(r'winner\\s+is\\s+[\"\\']?([AB]|Tie)[\"\\']?', eval_text, re.IGNORECASE)\n",
    "        if winner_fallback:\n",
    "            df_analysis.loc[index, 'prometheus_winner'] = winner_fallback.group(1).capitalize()\n",
    "        else:\n",
    "            winner_val == 'Parsing Error'\n",
    "            if winner_val == 'Parsing Error':\n",
    "                    match = re.search(r'\"winner\"\\s*:\\s*\"(Tie|A|B)\"', eval_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        winner_val = match.group(1)\n",
    "                    \n",
    "            df_analysis.loc[index, 'prometheus_winner'] = winner_val\n",
    "        \n",
    "        scores = []\n",
    "        for pattern in score_patterns + [r':\\s*(\\d+)\\s*\\([^)]*response']:\n",
    "            matches = re.findall(pattern, eval_text, re.IGNORECASE)\n",
    "            scores.extend([int(m) for m in matches])\n",
    "        \n",
    "        # Filter valid scores\n",
    "        scores = [s for s in scores if 0 <= s <= 5]\n",
    "\n",
    "        if not scores:\n",
    "            criteria_mentioned = sum([\n",
    "                1 for crit in ['logical_coherence', 'relevance_and_focus', \n",
    "                               'accuracy_and_truthfulness', 'conciseness_and_clarity']\n",
    "                if re.search(crit.replace('_', r'[_\\s]'), eval_text, re.IGNORECASE)\n",
    "            ])\n",
    "            if criteria_mentioned > 0:\n",
    "                positive_words = len(re.findall(r'\\b(comprehensive|detailed|coherent|well-structured|accurate|clear|concise|relevant|focused|logical|excellent|superior|better|good|strong|effective)\\b', eval_text, re.IGNORECASE))\n",
    "                negative_words = len(re.findall(r'\\b(not|lacking|poor|weak|unclear|inaccurate|irrelevant|inconsistent|insufficient|limited)\\b', eval_text, re.IGNORECASE))\n",
    "                if positive_words > negative_words:\n",
    "                    scores = [5] * criteria_mentioned\n",
    "                elif positive_words > 0:\n",
    "                    scores = [4] * criteria_mentioned\n",
    "                else:\n",
    "                    scores = [3] * criteria_mentioned\n",
    "\n",
    "            score = np.mean(scores) if scores else 0\n",
    "            if score == 0:\n",
    "                regex = r'\"score\"\\s*:\\s*(\\d+)'\n",
    "                scores = re.findall(regex, eval_text)\n",
    "                scores_numericos = [int(s) for s in scores]\n",
    "                score = np.mean(scores_numericos) if scores_numericos else 0\n",
    "\n",
    "        df_analysis.loc[index, 'prometheus_total_score'] = score\n",
    "\n",
    "    except Exception:\n",
    "        winner_val == 'Parsing Error'\n",
    "        if winner_val == 'Parsing Error':\n",
    "            match = re.search(r'\"winner\"\\s*:\\s*\"(Tie|A|B)\"', eval_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                winner_val = match.group(1)\n",
    "                    \n",
    "        df_analysis.loc[index, 'prometheus_winner'] = winner_val\n",
    "\n",
    "\n",
    "        score = 0\n",
    "        if score == 0:\n",
    "            regex = r'\"score\"\\s*:\\s*(\\d+)'\n",
    "            scores = re.findall(regex, eval_text)\n",
    "            scores_numericos = [int(s) for s in scores]\n",
    "            score = np.mean(scores_numericos) if scores_numericos else 0\n",
    "\n",
    "        df_analysis.loc[index, 'prometheus_total_score'] = score\n",
    "\n",
    "print(\"Prometheus processing completed.\")\n",
    "print(\"Sample results for Prometheus:\")\n",
    "df_analysis[['evaluation_id', 'prometheus_winner', 'prometheus_total_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "638ea5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count where 'prometheus_total_score' == 0: 1\n",
      "Count where 'prometheus_winner' is NaN: 0\n",
      "Count where 'prometheus_winner' == 'Parsing Error': 0\n"
     ]
    }
   ],
   "source": [
    "score_zero_count = (df_analysis['prometheus_total_score'] == 0).sum()\n",
    "print(f\"Count where 'prometheus_total_score' == 0: {score_zero_count}\")\n",
    "\n",
    "winner_nan_count = df_analysis['prometheus_winner'].isna().sum()\n",
    "print(f\"Count where 'prometheus_winner' is NaN: {winner_nan_count}\")\n",
    "\n",
    "parsing_error_count = (df_analysis['prometheus_winner'] == 'Parsing Error').sum()\n",
    "print(f\"Count where 'prometheus_winner' == 'Parsing Error': {parsing_error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c40e492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_winner = [col for col in df_analysis.columns if col.endswith('_winner')]\n",
    "df_analysis[colunas_winner] = df_analysis[colunas_winner].replace('T', 'Tie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a2bf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>response_A</th>\n",
       "      <th>response_B</th>\n",
       "      <th>evaluation_claude</th>\n",
       "      <th>evaluation_prometheus</th>\n",
       "      <th>evaluation_mistral</th>\n",
       "      <th>claude_winner</th>\n",
       "      <th>claude_total_score</th>\n",
       "      <th>mistral_winner</th>\n",
       "      <th>mistral_total_score</th>\n",
       "      <th>prometheus_winner</th>\n",
       "      <th>prometheus_total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>{\\n    \"winner\": \"A\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"T\",\\n\"general_justification\": \"T...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "      <td>A</td>\n",
       "      <td>4.50</td>\n",
       "      <td>A</td>\n",
       "      <td>4.75</td>\n",
       "      <td>Tie</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>**Notação Big O: uma medida de complexidade**\\...</td>\n",
       "      <td>A notação Big O é uma medida de complexidade d...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"Tie\",\\n\"general_justification\": ...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "      <td>B</td>\n",
       "      <td>4.75</td>\n",
       "      <td>A</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Tie</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CG004_gemini-1.5-pro-latest_1_General Knowledg...</td>\n",
       "      <td>A Proclamação da República no Brasil em 15 de ...</td>\n",
       "      <td>A Proclamação da República no Brasil, em 15 de...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"T\",\\n\"general_justification\": \"B...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "      <td>B</td>\n",
       "      <td>5.00</td>\n",
       "      <td>A</td>\n",
       "      <td>4.75</td>\n",
       "      <td>Tie</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...   \n",
       "2  CG004_gemini-1.5-pro-latest_1_General Knowledg...   \n",
       "\n",
       "                                          response_A  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  **Notação Big O: uma medida de complexidade**\\...   \n",
       "2  A Proclamação da República no Brasil em 15 de ...   \n",
       "\n",
       "                                          response_B  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  A notação Big O é uma medida de complexidade d...   \n",
       "2  A Proclamação da República no Brasil, em 15 de...   \n",
       "\n",
       "                                   evaluation_claude  \\\n",
       "0  {\\n    \"winner\": \"A\",\\n    \"general_justificat...   \n",
       "1  {\\n    \"winner\": \"B\",\\n    \"general_justificat...   \n",
       "2  {\\n    \"winner\": \"B\",\\n    \"general_justificat...   \n",
       "\n",
       "                               evaluation_prometheus  \\\n",
       "0  {\\n\"winner\": \"T\",\\n\"general_justification\": \"T...   \n",
       "1  {\\n\"winner\": \"Tie\",\\n\"general_justification\": ...   \n",
       "2  {\\n\"winner\": \"T\",\\n\"general_justification\": \"B...   \n",
       "\n",
       "                                  evaluation_mistral claude_winner  \\\n",
       "0   {\\n        \"winner\": \"A\",\\n        \"general_j...             A   \n",
       "1   {\\n        \"winner\": \"A\",\\n        \"general_j...             B   \n",
       "2   {\\n        \"winner\": \"A\",\\n        \"general_j...             B   \n",
       "\n",
       "   claude_total_score mistral_winner  mistral_total_score prometheus_winner  \\\n",
       "0                4.50              A                 4.75               Tie   \n",
       "1                4.75              A                 5.00               Tie   \n",
       "2                5.00              A                 4.75               Tie   \n",
       "\n",
       "   prometheus_total_score  \n",
       "0                    5.00  \n",
       "1                    4.75  \n",
       "2                    4.75  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c33fe6",
   "metadata": {},
   "source": [
    "### committee judges consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db148e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consensus(row):\n",
    "    \"\"\"\n",
    "    Analisa os vencedores de cada juiz e retorna o consenso por maioria.\n",
    "    \"\"\"\n",
    "    votes = [\n",
    "        row['claude_winner'],\n",
    "        row['mistral_winner'],\n",
    "        row['prometheus_winner']\n",
    "    ]\n",
    "    \n",
    "    valid_votes = [vote for vote in votes if vote != 'Parsing Error']\n",
    "    if len(valid_votes) < 2:\n",
    "        return 'No Consensus'\n",
    "        \n",
    "\n",
    "    vote_counts = Counter(valid_votes)\n",
    "    \n",
    "    for winner, count in vote_counts.items():\n",
    "        if count >= 2:\n",
    "            return winner\n",
    "\n",
    "    return 'Tie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c11c988e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>response_A</th>\n",
       "      <th>response_B</th>\n",
       "      <th>evaluation_claude</th>\n",
       "      <th>evaluation_prometheus</th>\n",
       "      <th>evaluation_mistral</th>\n",
       "      <th>claude_winner</th>\n",
       "      <th>claude_total_score</th>\n",
       "      <th>mistral_winner</th>\n",
       "      <th>mistral_total_score</th>\n",
       "      <th>prometheus_winner</th>\n",
       "      <th>prometheus_total_score</th>\n",
       "      <th>gold_winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CG011_llama-3.3-70b-versatile_2_General Knowle...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>**O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...</td>\n",
       "      <td>{\\n    \"winner\": \"A\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"T\",\\n\"general_justification\": \"T...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "      <td>A</td>\n",
       "      <td>4.50</td>\n",
       "      <td>A</td>\n",
       "      <td>4.75</td>\n",
       "      <td>Tie</td>\n",
       "      <td>5.00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC018_llama-3.3-70b-versatile_3_Technical_mini...</td>\n",
       "      <td>**Notação Big O: uma medida de complexidade**\\...</td>\n",
       "      <td>A notação Big O é uma medida de complexidade d...</td>\n",
       "      <td>{\\n    \"winner\": \"B\",\\n    \"general_justificat...</td>\n",
       "      <td>{\\n\"winner\": \"Tie\",\\n\"general_justification\": ...</td>\n",
       "      <td>{\\n        \"winner\": \"A\",\\n        \"general_j...</td>\n",
       "      <td>B</td>\n",
       "      <td>4.75</td>\n",
       "      <td>A</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Tie</td>\n",
       "      <td>4.75</td>\n",
       "      <td>Tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       evaluation_id  \\\n",
       "0  CG011_llama-3.3-70b-versatile_2_General Knowle...   \n",
       "1  TC018_llama-3.3-70b-versatile_3_Technical_mini...   \n",
       "\n",
       "                                          response_A  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  **Notação Big O: uma medida de complexidade**\\...   \n",
       "\n",
       "                                          response_B  \\\n",
       "0  **O que é DNA?**\\n\\nO DNA (ácido desoxirribonu...   \n",
       "1  A notação Big O é uma medida de complexidade d...   \n",
       "\n",
       "                                   evaluation_claude  \\\n",
       "0  {\\n    \"winner\": \"A\",\\n    \"general_justificat...   \n",
       "1  {\\n    \"winner\": \"B\",\\n    \"general_justificat...   \n",
       "\n",
       "                               evaluation_prometheus  \\\n",
       "0  {\\n\"winner\": \"T\",\\n\"general_justification\": \"T...   \n",
       "1  {\\n\"winner\": \"Tie\",\\n\"general_justification\": ...   \n",
       "\n",
       "                                  evaluation_mistral claude_winner  \\\n",
       "0   {\\n        \"winner\": \"A\",\\n        \"general_j...             A   \n",
       "1   {\\n        \"winner\": \"A\",\\n        \"general_j...             B   \n",
       "\n",
       "   claude_total_score mistral_winner  mistral_total_score prometheus_winner  \\\n",
       "0                4.50              A                 4.75               Tie   \n",
       "1                4.75              A                 5.00               Tie   \n",
       "\n",
       "   prometheus_total_score gold_winner  \n",
       "0                    5.00           A  \n",
       "1                    4.75         Tie  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis['gold_winner'] = df_analysis.apply(find_consensus, axis=1)\n",
    "df_analysis.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37c10978",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.to_csv('../../data/judged/committee_llm_consensus_lingual_pt.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
