import pandas as pd
import time
import anthropic
import os
import json

INPUT_CSV = '../../data/processed/sample_master.csv'
OUTPUT_CSV = '../../data/judged/sample_master_claude_judge.csv'

ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
ESSENTIAL_COLS = [
    'evaluation_id', 'response_A', 'response_B'
]

anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

def judge_claude(prompt):
    try:
        response = anthropic_client.messages.create(
            model="claude-sonnet-4-20250514",
            messages=prompt,
            max_tokens=1024,
            temperature=0
        )
        return response.content[0].text
    except Exception as e:
        return json.dumps({"winner": "Error", "error_details": str(e)})

def create_judge_prompt(response_a, response_b):
    core_prompt = f"""
    You are an expert judge specializing in evaluating the quality of responses generated by language models.
    Your task is to compare two responses (Response A and Response B) objectively and impartially,
    based on the provided rubric. 
    
    IMPORTANT RULES:
    - **Blind Evaluation:** You do not know which model generated each response. Evaluate only the content presented. 
    - **Mandatory Evidence:** For EACH criterion, you MUST quote a brief excerpt from the response that justifies your 
        score. This is crucial. 
    - **Length-Blind Independence:** The length of the response should NOT influence your evaluation, unless it 
        negatively affects "Conciseness and Clarity". Do not penalize a response for being short and direct,
        nor reward one for being long and detailed if the extra information is not relevant. 
    - **Output Format:** Your response must be STRICTLY a JSON object, with no additional text before or after. 
        
    EVALUATION CRITERIA: 
    1. **Logical Coherence (0-5):** Is the response logically consistent? Does the reasoning flow without contradictions? 
    2. **Relevance and Focus (0-5):** Does the response directly address the requested question or task, without straying
        into irrelevant information? 3. **Accuracy and Truthfulness (0-5):** Is the information presented factually 
        correct and reliable? Does the response avoid hallucinations or misinformation? 
    4. **Conciseness and Clarity (0-5):** Is the response presented in a clear, direct, and easy-to-understand manner? 
        Does it avoid unnecessary jargon and verbosity? **TASK:** Evaluate the two responses below. 
        
    [RESPONSE A]
    {response_a} 
    [/RESPONSE A] 
    
    [RESPONSE B]
    {response_b}
    [/RESPONSE B] 
    
    Based on your analysis, fill out the following JSON format. Provide a score from 0 to 5 for each criterion. 
    
    {{ 
        "winner": "A", "B", or "Tie", 
        "general_justification": "A concise 1-2 sentence explanation of why you made your choice.", 
        "criteria": {{
            "logical_coherence": {{ 
                "score": <0-5>, 
                "justification": "<Your justification quoting an excerpt from the response.>" 
            }}, 
            "relevance_and_focus": {{ 
                "score": <0-5>, 
                "justification": "<Your justification quoting an excerpt from the response.>" 
            }}, 
            "accuracy_and_truthfulness": {{ 
                "score": <0-5>, 
                "justification": "<Your justification quoting an excerpt from the response.>" 
            }}, 
            "conciseness_and_clarity": {{ 
                "score": <0-5>, 
                "justification": "<Your justification quoting an excerpt from the response.>" 
            }} 
        }} 
    }}
    """

    return [
        {"role": "user", "content": core_prompt}
    ]

df = pd.read_csv(INPUT_CSV)
df_eval = df[ESSENTIAL_COLS].copy()

# Shuffle
df_eval = df_eval.sample(frac=1, random_state=42).reset_index(drop=True)

df_eval['evaluation_claude'] = None
print(f"Loaded {len(df_eval)} pairs for evaluation.")

evaluated_ids = set()
if os.path.exists(OUTPUT_CSV):
    df_existing = pd.read_csv(OUTPUT_CSV)
    evaluated_ids = set(df_existing['evaluation_id'])
    print(f"Found existing file: {len(evaluated_ids)} already evaluated.")

for idx, row in df_eval.iterrows():
    eval_id = row['evaluation_id']
    if eval_id in evaluated_ids:
        continue

    response_a = str(row['response_A'])
    response_b = str(row['response_B'])

    if not response_a or not response_b:
        continue

    prompt = create_judge_prompt(response_a, response_b)
    print(f"Evaluating pair {idx+1}/{len(df_eval)}...")
    output = judge_claude(prompt)

    new_row = {
        'evaluation_id': eval_id,
        'response_A': response_a,
        'response_B': response_b,
        'evaluation_claude': output
    }

    pd.DataFrame([new_row]).to_csv(
        OUTPUT_CSV,
        mode='a',
        header=not os.path.exists(OUTPUT_CSV),
        index=False,
        encoding='utf-8-sig'
    )

    time.sleep(1)

print(f"All results saved to '{OUTPUT_CSV}'")
